{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4feb55ce-e05c-4c39-8d71-565842d5e902",
   "metadata": {},
   "source": [
    "# PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f169258-a40a-4487-9ff5-be4c2ee91090",
   "metadata": {},
   "source": [
    "This lectures provides an overview of PySpark.\n",
    "\n",
    "There are two ways to run the content in this server: connect to a remote server or run via a local instance. These options can be controlled via the variable `USE_REMOTE`. \n",
    "\n",
    "If you want to run your code directly via a .py file, you would use the following command:\n",
    "\n",
    "```python\n",
    "spark-submit --master spark://your-server-ip:7077 your_script.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7999f803-e440-4739-b4fc-3f09256e5a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If connecting to a remote server\n",
    "# !pip install \"pyspark[connect]\"==4.0.1 # very lightweight\n",
    "\n",
    "# If connecting to a local, dev, instance\n",
    "#!pip install pyspark==4.0.1 # bigger download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd843faa-b31a-4580-8bf7-4f7dfae21c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9399195b-5414-40b6-b50e-c64edf0ffd2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remote server\n",
    "MASTER_IP = \"51.222.140.217\"\n",
    "REMOTE_SERVER = f\"sc://{MASTER_IP}/15002\"\n",
    "\n",
    "USE_REMOTE = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f40037-eabc-4ec2-bd9b-7bf99c83e8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Dashboard available at http://{MASTER_IP}:8080/\" if USE_REMOTE else f\"Dashboard available at http://localhost:4040/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b009b0e-d18f-48dc-98a7-cc1acf00f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that spark.sql.ansi.enabled is set to false to allow 'Pandas on Spark' to work correctly later in the lecture\n",
    "if USE_REMOTE:\n",
    "    spark = SparkSession.builder.remote(MASTER_URL).appName(f\"job_id_{getpass.getuser()}\").config(\"spark.sql.ansi.enabled\", \"false\").getOrCreate()\n",
    "else:\n",
    "    spark = SparkSession.builder.master(\"local[*]\").appName(f\"job_id_{getpass.getuser()}\").config(\"spark.sql.ansi.enabled\", \"false\") .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230e2fe5-3a37-4f20-94ad-e928d3ace511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test it\n",
    "df = spark.range(1000)\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659004c-aefe-4e87-9fbb-b3074c9797cc",
   "metadata": {},
   "source": [
    "Notes:\n",
    "\n",
    "Remote `connect server` for Spark may be started via `/opt/spark/sbin/start-connect-server.sh`\n",
    "\n",
    "If no remote installation exists, the following script will install it:\n",
    "```bash\n",
    "wget https://archive.apache.org/dist/spark/spark-4.0.1/spark-4.0.1-bin-hadoop3.tgz\n",
    "tar -zxf spark-4.0.1-bin-hadoop3.tgz \n",
    "sudo mv spark-4.0.1-bin-hadoop3 /opt/spark\n",
    "rm spark-4.0.1-bin-hadoop3.tgz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56c1dd9-8436-4403-a02a-8bfedf6730cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68091908-42d0-4305-a5f7-4c365a9b9c2d",
   "metadata": {},
   "source": [
    "## Spark distributes data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9999f44-42de-44b5-bd0d-76ca78db2950",
   "metadata": {},
   "source": [
    "Read via Pandas for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d110afad-eef9-4be0-a3b7-ebbdff365ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trades_df = pd.read_csv(\"../../datasets/market_data/trades_2025-09-10_AAPLMSFT_sorted.csv.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834fcea8-046b-40df-a87b-485a31f988f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(trades_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622746b-aa5a-4ce5-83a9-4c79b33b6898",
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad143e4e-40e8-474c-9614-72625bee522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "len(trades_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28751e2-e342-4b60-877d-06537006ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del trades_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e21da6b-0599-4647-ad03-b05c06b98e0c",
   "metadata": {},
   "source": [
    "Now read via Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787decd6-d66a-49bd-b528-5a7d1be29f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trades_df = spark.read.csv(\"../../datasets/market_data/trades_2025-09-10_AAPLMSFT_sorted.csv.gz\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e84946-6356-4d3f-9082-cde15bb67ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(trades_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb35f6a-1d1f-458a-9dd4-31c6be4ddcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8408db2c-8270-448c-886e-e93471f2947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trades_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efed84a-74f8-420b-af83-98487859121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trades_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6ee9d3-317c-4040-8d88-49b430d077f9",
   "metadata": {},
   "source": [
    "Note that Spark data is generally distributed and operations on it are also distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccce3d0-ba41-4c58-b62e-59865f4963de",
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b6d6bf-80b6-4bea-8f0a-fabaacffd2fa",
   "metadata": {},
   "source": [
    "#### Pick specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcae5464-0252-4556-8ccc-b957a9e5354b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_df.select('ticker', 'price', 'size').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d6ef5-bd5d-4039-b863-0dc7d5953f63",
   "metadata": {},
   "source": [
    "#### Pick rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f2e3c-a057-4729-ba5f-866de87815da",
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_df.filter(trades_df.size > 200).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b31f9a-cfb4-4d4b-8fc7-a15fbf429d8e",
   "metadata": {},
   "source": [
    "#### Many operations in Spark are **lazy**, they don't execute until they are actually needed\n",
    "\n",
    "This allows Spark to look at several operatoins together and possibly optimize them  \n",
    "Notice the difference in execution speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac7cfc6-6e41-4ce1-b84f-ff053158e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trades_df.groupby('ticker').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9c79b7-f2cb-4779-9857-e7b2bbf6fbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "trades_df.groupby('ticker').mean().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7513aecc-244e-4b1b-8ad2-bd24399a66ab",
   "metadata": {},
   "source": [
    "Generally speaking, function calls, such as `filter` or `select` don't actually transform the data! Only when data is actually demanded, by functions such as `show` or `collect`, does Pyspark optimize the built up commands and executes them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd76797d-ec55-4c09-8ba4-15b8eba5911d",
   "metadata": {},
   "source": [
    "#### Spark DataFrames are built on top of RDD (Resilient Distributed Datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9777c6b-e6eb-4639-bdef-6acebd629e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(trades_df.rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ccc28f-d8c7-4b1b-a7f8-ca951aec6851",
   "metadata": {},
   "source": [
    "An RDD is assumed to be distributed by default. It is a low level datastructure which is no longer used directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b092b14f-2b96-4e5d-9018-630c020ef279",
   "metadata": {},
   "source": [
    "### SQL is built into Spark!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b3c435-7c1d-4851-8c5c-d2de4b3a7a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_df.createOrReplaceTempView(\"trades_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fde0aa-a6ce-49da-a9ec-05c2b3177937",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT ticker, mean(price), mean(size) from trades_df group by ticker\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2eaafe-21c2-4544-9de9-3bbf8e3fab14",
   "metadata": {},
   "source": [
    "### ... actually Pandas is also built into Spark\n",
    "(although it is not as featureful as Spark's built-in Dataframes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de121161-c10e-46a8-bacc-3a730d634387",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9032c3e4-b4e8-4cc6-ac35-29625a741337",
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_ds = ps.read_csv(\"../../datasets/market_data/trades_2025-09-10_AAPLMSFT_sorted.csv.gz\")\n",
    "trades_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a38c6bf-02be-4465-85f7-5ea010c4f07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(trades_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fc727d-d09b-4317-9fbc-201227a3ca18",
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_ds.ticker.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220ca52d-f80d-4d01-a30b-3cfcbe27db36",
   "metadata": {},
   "source": [
    "## Machine learning in PySpark\n",
    "Think of this as distributed scikit-learn!\n",
    "\n",
    "Note that PySpark has has a set of libraries under `pyspark.mllib.*` and `pyspark.ml.*`. The _mllib_ set of packages are in maintenance mode. They were designed to work with RDDs, a predecessor to Dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5f4457-8716-4d9b-8f34-0370af571529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "#only keep numeric columns (for simplicity)\n",
    "titanic_onlynum_df = titanic_df.drop(['sex', 'embarked', 'class', 'who', 'deck', 'embark_town', 'alive'], axis=1)\n",
    "\n",
    "#remove na, nan, etc.\n",
    "titanic_onlynum_noempty_df = titanic_onlynum_df.dropna()\n",
    "titanic_df = spark.createDataFrame(titanic_onlynum_noempty_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc2aa97-d6df-4f22-af29-eb2d909317c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe3de08-243d-4f62-b746-43aa4a729982",
   "metadata": {},
   "source": [
    "_pclass_: Ticket class (1st, 2nd, 3rd)  \n",
    "_sibsp_: Number of siblings/spouses on board  \n",
    "_parch_: Number of parents/children onboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf81af3-b99d-40ca-8c9c-6a7e6ec83e06",
   "metadata": {},
   "source": [
    "Cast booleans to integers  \n",
    "(recall that scikit-learn was able to work with `bool` values directly, could it be because PySpark is actually a layer on top of Scala, which is a layer on top of Java, which does not have a way to automatically translate booleans to integers?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afa37ae-2e60-45e0-9f58-d0d3e2040794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, col\n",
    "\n",
    "titanic_df = titanic_df.withColumn('adult_male', col('adult_male').cast('int')).withColumn('alone', col('alone').cast('int'))\n",
    "titanic_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e72bd6-6920-4069-853d-d07d5a87d17e",
   "metadata": {},
   "source": [
    "Recall that in scikit-learn, the `.fit()` function takes two arguments for supervised learning algorithms `X` and `y`. Pyspark does things differently. It wants a single dataframe where the features are a single column and the target is a separate column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab60716-f216-4baf-81f7-d40d6e8c7565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "feature_cols = ['pclass', 'age', 'sibsp', 'parch', 'fare', 'adult_male', 'alone']\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = feature_columns,\n",
    "    outputCol = \"features\"\n",
    ")\n",
    "\n",
    "assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ce7c2-3b08-49d0-a6de-7974847c783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_ml_df = assembler.transform(titanic_df).drop(*feature_cols)\n",
    "\n",
    "titanic_ml_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb23b4-d732-4bb4-ac76-cc8a6fe625f1",
   "metadata": {},
   "source": [
    "#### Split test/train\n",
    "\n",
    "Surprisingly, _many_ pyspark examples in their official documentation or online blogs don't do a test/train split!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8378ab1e-8e54-4fd3-a909-f17cfda0e621",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, test_data) = titanic_ml_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "train_data.count(), test_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd627856-8bb5-4323-9e95-8c27322beb09",
   "metadata": {},
   "source": [
    "#### Pyspark also has the concept of pipelines\n",
    "\n",
    "(although we don't use it here)\n",
    "\n",
    "```python\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, decision_tree_classifier])\n",
    "```\n",
    "\n",
    "Example taken from https://spark.apache.org/docs/latest/ml-classification-regression.html#logistic-regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ac19a-ceda-4ba2-a237-6dd1f6d35da8",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd978e06-4128-4b1e-90ce-7c39d6ba581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(featuresCol=\"features\", labelCol=\"survived\").fit(train_data)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79c4336-8598-4b14-8e3f-4b997051998e",
   "metadata": {},
   "source": [
    "#### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb83b027-ee15-40e1-be58-c34e60a01b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cac509-20d6-4f71-b07e-d1c4058a977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf59f3-f244-4c9b-82ee-eba3c95ad4e1",
   "metadata": {},
   "source": [
    "#### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc42fd-1d5d-4a1c-be3d-d20387a5d77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "#from pyspark.mllib.evaluation import BinaryClassificationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72376fbf-30b7-4b08-b1c0-16417e4a8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"survived\", metricName=\"areaUnderPR\")\n",
    "\n",
    "eval_metric = evaluator.evaluate(predictions)\n",
    "eval_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e079ef-4d93-4392-be12-4dde8778bc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
